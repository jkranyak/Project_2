{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "60de5cf8",
      "metadata": {
        "id": "60de5cf8"
      },
      "source": [
        "# **_Stroke Prediction Project_**\n",
        "\n",
        "Sirisha Mandava, Jeff Boczkaja, Mohamed Altoobli, Jesse Kranyak"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0985550d",
      "metadata": {
        "id": "0985550d"
      },
      "source": [
        "Utilizing the Stroke Prediction Dataset from Kaggle we set out to make a machine learning program that will be able to accurately predict whether or not someone will have a stroke. We try out different models that provided us with varying results. We show our results using a few different metrics including balanced accuracy score, F1 scores, precision, and recall.\n",
        "\n",
        "Source: https://www.kaggle.com/datasets/fedesoriano/stroke-prediction-dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ac93bc70",
      "metadata": {
        "id": "ac93bc70"
      },
      "source": [
        "## What do the metrics measure?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e126120a",
      "metadata": {
        "id": "e126120a"
      },
      "source": [
        "### <u>Precision</u>\n",
        "Precision measures the accuracy of positive predictions. It is the ratio of true positive predictions to the total number of positive predictions made. In other words, it answers the question, \"Of all the instances the model predicted as positive, how many are actually positive?\" Precision is particularly important in scenarios where the cost of a false positive is high.\n",
        "\n",
        "Formula: Precision = True Positives / (True Positives + False Positives)\n",
        "\n",
        "### <u>Recall</u>\n",
        "Recall, also known as sensitivity or true positive rate, measures the ability of a model to find all the relevant cases within a dataset. It is the ratio of true positive predictions to the total number of actual positives. Recall answers the question, \"Of all the actual positives, how many did the model successfully identify?\" Recall is crucial in situations where missing a positive instance is costly.\n",
        "\n",
        "Formula: Recall = True Positives / (True Positives + False Negatives)\n",
        "\n",
        "### <u>F1 Score</u>\n",
        "The F1 Score is the mean of precision and recall. It provides a single metric that balances both the precision and recall of a classification model, which is particularly useful when you want to compare two or more models. The F1 Score is especially valuable when the distribution of class labels is imbalanced. A high F1 Score indicates that the model has low false positives and low false negatives, so it's correctly identifying real positives and negatives.\n",
        "\n",
        "Formula: F1 Score = 2 * (Precision * Recall) / (Precision + Recall)\n",
        "\n",
        "### <u>Balanced Accuracy Score</u>\n",
        "Balanced Accuracy Score is defined as the average of recall obtained on each class, meaning it considers both the true positive rate and the true negative rate. It calculates the accuracy of the model by taking into account the balance between classes. For a binary classification problem, it would be the average of the proportion of correctly predicted positive observations to the total positive observations and the proportion of correctly predicted negative observations to the total negative observations.\n",
        "\n",
        "Formula: Balanced Accuracy Score = (1/2) * ((TP / (TP + FN)) + (TN / (TN + FP)))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install optuna\n",
        "!pip install --upgrade tensorflow-datasets tensorflow-hub tensorflow-io-gcs-filesystem tensorflow-metadata tensorflow-probability\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NMpXvyoZDIQ5",
        "outputId": "91d727d6-ebb5-4559-ab96-324e9cdf2591"
      },
      "id": "NMpXvyoZDIQ5",
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: optuna in /usr/local/lib/python3.10/dist-packages (3.6.1)\n",
            "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (1.13.1)\n",
            "Requirement already satisfied: colorlog in /usr/local/lib/python3.10/dist-packages (from optuna) (6.8.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from optuna) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (24.0)\n",
            "Requirement already satisfied: sqlalchemy>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (2.0.29)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from optuna) (4.66.2)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from optuna) (6.0.1)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna) (1.3.3)\n",
            "Requirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna) (4.11.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy>=1.3.0->optuna) (3.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from Mako->alembic>=1.5.0->optuna) (2.1.5)\n",
            "Requirement already satisfied: tensorflow-datasets in /usr/local/lib/python3.10/dist-packages (4.9.4)\n",
            "Requirement already satisfied: tensorflow-hub in /usr/local/lib/python3.10/dist-packages (0.16.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem in /usr/local/lib/python3.10/dist-packages (0.36.0)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.10/dist-packages (1.15.0)\n",
            "Requirement already satisfied: tensorflow-probability in /usr/local/lib/python3.10/dist-packages (0.24.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets) (1.4.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets) (8.1.7)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets) (0.1.8)\n",
            "Requirement already satisfied: etils[enp,epath,etree]>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets) (1.7.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets) (1.25.2)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets) (2.3)\n",
            "Requirement already satisfied: protobuf>=3.20 in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets) (3.20.3)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets) (5.9.5)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets) (2.31.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets) (2.4.0)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets) (0.10.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets) (4.66.2)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets) (1.14.1)\n",
            "Requirement already satisfied: array-record>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets) (0.5.1)\n",
            "Requirement already satisfied: tf-keras>=2.14.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow-hub) (2.15.1)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-probability) (1.16.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from tensorflow-probability) (4.4.2)\n",
            "Requirement already satisfied: cloudpickle>=1.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow-probability) (2.2.1)\n",
            "Requirement already satisfied: gast>=0.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow-probability) (0.5.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from etils[enp,epath,etree]>=0.9.0->tensorflow-datasets) (2023.6.0)\n",
            "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.10/dist-packages (from etils[enp,epath,etree]>=0.9.0->tensorflow-datasets) (6.4.0)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.10/dist-packages (from etils[enp,epath,etree]>=0.9.0->tensorflow-datasets) (4.11.0)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.10/dist-packages (from etils[enp,epath,etree]>=0.9.0->tensorflow-datasets) (3.18.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->tensorflow-datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->tensorflow-datasets) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->tensorflow-datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->tensorflow-datasets) (2024.2.2)\n",
            "Requirement already satisfied: tensorflow<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tf-keras>=2.14.1->tensorflow-hub) (2.15.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow-hub) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow-hub) (24.3.25)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow-hub) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow-hub) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow-hub) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow-hub) (0.2.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow-hub) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow-hub) (24.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow-hub) (67.7.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow-hub) (1.62.2)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow-hub) (2.15.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow-hub) (2.15.0)\n",
            "Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow-hub) (2.15.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow-hub) (0.43.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow-hub) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow-hub) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow-hub) (3.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow-hub) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow-hub) (3.0.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow-hub) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow-hub) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow-hub) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow-hub) (1.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow-hub) (2.1.5)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow-hub) (0.6.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow-hub) (3.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U imbalanced-learn\n",
        "!pip install -U scikit-learn imbalanced-learn\n",
        "!pip install -U scikit-learn\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kY2vLCXhFpwi",
        "outputId": "af830ccd-067f-4701-b981-b5e2f536a1c2"
      },
      "id": "kY2vLCXhFpwi",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.10/dist-packages (0.12.2)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.11.4)\n",
            "Requirement already satisfied: scikit-learn>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.4.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.4.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (3.4.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.4.2)\n",
            "Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.10/dist-packages (0.12.2)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.4.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.4.2)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.4.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a7138166",
      "metadata": {
        "id": "a7138166"
      },
      "source": [
        "# Main Project"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8954c351",
      "metadata": {
        "id": "8954c351"
      },
      "source": [
        "## 1. Importing Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "429c0f2b",
      "metadata": {
        "id": "429c0f2b"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.over_sampling import SMOTENC\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import balanced_accuracy_score, classification_report, confusion_matrix, precision_recall_fscore_support\n",
        "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import RandomizedSearchCV"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"healthcare-dataset-stroke-data.csv\")\n",
        "df"
      ],
      "metadata": {
        "id": "tfkpXBwlSHxu"
      },
      "id": "tfkpXBwlSHxu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "4212b2a2",
      "metadata": {
        "id": "4212b2a2"
      },
      "source": [
        "## 2. Analyzing and Exploring our Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5217dd5b",
      "metadata": {
        "scrolled": true,
        "id": "5217dd5b"
      },
      "outputs": [],
      "source": [
        "#Lets go ahead and loop through all of our columns and see what data they reveal\n",
        "\n",
        "def describe_df(df: pd.DataFrame):\n",
        "    print(f\"The dataset contains {df.shape[1]} columns and {len(df)} rows\")\n",
        "    for col in df.columns:\n",
        "        col_dtype = df[col].dtype\n",
        "        print(f\"\\nColumn: {col} ({col_dtype})\")\n",
        "        if col_dtype == 'object':\n",
        "            print(f\"--- Percentage of NaNs: {df[col].isna().sum() / len(df[col]) * 100}\")\n",
        "            print(f\"--- Unique values:\\n {df[col].unique()}\")\n",
        "        else:\n",
        "            print(f\"--- Summary statistics:\\n {df[col].describe()}\")\n",
        "describe_df(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ea5170a7",
      "metadata": {
        "id": "ea5170a7"
      },
      "source": [
        "### Check balance of our target which is 'stroke'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aaf00f2a",
      "metadata": {
        "id": "aaf00f2a"
      },
      "outputs": [],
      "source": [
        "df['stroke'].value_counts() # We have pretty imbalanced data!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "66bcf7f5",
      "metadata": {
        "id": "66bcf7f5"
      },
      "source": [
        "### Drop unneeded column of 'id'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "35c7065f",
      "metadata": {
        "id": "35c7065f"
      },
      "outputs": [],
      "source": [
        "df = df.drop('id', axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0f744fec",
      "metadata": {
        "id": "0f744fec"
      },
      "source": [
        "### Check nulls and use Imputation to replace them"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a7a5e072",
      "metadata": {
        "id": "a7a5e072"
      },
      "outputs": [],
      "source": [
        "df.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_original = df.copy()"
      ],
      "metadata": {
        "id": "-yssu1ajSENT"
      },
      "id": "-yssu1ajSENT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c9642c4b",
      "metadata": {
        "id": "c9642c4b"
      },
      "outputs": [],
      "source": [
        "from sklearn.impute import SimpleImputer\n",
        "import pandas as pd\n",
        "\n",
        "# Create an imputer object with a mean filling strategy\n",
        "mean_imputer = SimpleImputer(strategy='mean')\n",
        "\n",
        "# Apply the imputer to the 'bmi' column\n",
        "df['bmi'] = mean_imputer.fit_transform(df[['bmi']])\n",
        "\n",
        "# Check if any null values remain\n",
        "print(df['bmi'].isnull().sum())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets see if that was the best method"
      ],
      "metadata": {
        "id": "fK7P2D5VR0rZ"
      },
      "id": "fK7P2D5VR0rZ"
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"After Imputation:\")\n",
        "print(df['bmi'].describe())\n",
        "print(\"Before Imputation:\")\n",
        "print(df_original['bmi'].describe())"
      ],
      "metadata": {
        "id": "s4sMGxF_R0zq"
      },
      "id": "s4sMGxF_R0zq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.histplot(df_original['bmi'], color=\"red\", label=\"Before Imputation\", kde=True, stat=\"density\", linewidth=0)\n",
        "sns.histplot(df['bmi'], color=\"blue\", label=\"After Imputation\", kde=True, stat=\"density\", linewidth=0)\n",
        "plt.legend(title=\"BMI Distribution\")\n",
        "plt.title(\"Comparison of BMI Distribution Before and After Imputation\")\n",
        "plt.xlabel(\"BMI\")\n",
        "plt.ylabel(\"Density\")\n",
        "plt.show()\n",
        "#"
      ],
      "metadata": {
        "id": "TAu2mHczTFdI"
      },
      "id": "TAu2mHczTFdI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "adfb6453",
      "metadata": {
        "scrolled": false,
        "id": "adfb6453"
      },
      "outputs": [],
      "source": [
        "# Set the aesthetic style of the plots\n",
        "sns.set_style(\"whitegrid\")\n",
        "\n",
        "# Plotting the distribution of ages\n",
        "plt.figure(figsize=(10, 5))\n",
        "sns.histplot(df['age'], bins=30, kde=True, color=\"skyblue\")\n",
        "plt.title('Distribution of Ages')\n",
        "plt.xlabel('Age')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3e259e3d",
      "metadata": {
        "id": "3e259e3d"
      },
      "source": [
        "### Heatmap of Numerical Factors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "89c85d84",
      "metadata": {
        "id": "89c85d84"
      },
      "outputs": [],
      "source": [
        "df_corr = df.drop(['gender', 'ever_married', 'work_type', 'Residence_type', 'smoking_status'], axis=1)\n",
        "\n",
        "corr = df_corr.corr()\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(corr, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
        "plt.title('Correlation between Individual Factors and Stroke')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e50920b8",
      "metadata": {
        "id": "e50920b8"
      },
      "source": [
        "### Glucose Levels by Different Age Groups"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6d0d3676",
      "metadata": {
        "id": "6d0d3676"
      },
      "outputs": [],
      "source": [
        "# Creating age groups\n",
        "df['age_group'] = pd.cut(df['age'], bins=[0, 18, 30, 40, 50, 60, 70, 80, 90, 100], labels=['0-18', '19-30', '31-40', '41-50', '51-60', '61-70', '71-80', '81-90', '91-100'])\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.boxplot(x='age_group', y='avg_glucose_level', data=df, palette=\"coolwarm\")\n",
        "plt.title('Distribution of Average Glucose Levels Across Different Age Groups')\n",
        "plt.xlabel('Age Group')\n",
        "plt.ylabel('Average Glucose Level')\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "528e429c",
      "metadata": {
        "id": "528e429c"
      },
      "source": [
        "## Who is having the strokes?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ffb291a3",
      "metadata": {
        "id": "ffb291a3"
      },
      "outputs": [],
      "source": [
        "# Calculate the minimum age of someone who had a stroke\n",
        "min_age_stroke = df[df['stroke'] == 1]['age'].min()\n",
        "print(f'Youngest person in data with stroke: {min_age_stroke} years')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a41f6088",
      "metadata": {
        "id": "a41f6088"
      },
      "outputs": [],
      "source": [
        "# # Assuming 'df' is your DataFrame containing the stroke data\n",
        "# min_age_stroke = df[df['stroke'] == 1]['age'].min()\n",
        "\n",
        "under_50_stroke = df[(df['age'] < 50) & (df['stroke'] == 1)].shape[0]\n",
        "total_under_50 = df[df['age'] < 50].shape[0]\n",
        "percentage_under_50_stroke = (under_50_stroke / total_under_50) * 100\n",
        "\n",
        "over_50_stroke = df[(df['age'] >= 50) & (df['stroke'] == 1)].shape[0]\n",
        "total_over_50 = df[df['age'] >= 50].shape[0]\n",
        "percentage_over_50_stroke = (over_50_stroke / total_over_50) * 100\n",
        "\n",
        "# Pie Charts for strokes based on age\n",
        "labels = ['Had Stroke', 'No Stroke']\n",
        "sizes_under_50 = [percentage_under_50_stroke, 100 - percentage_under_50_stroke]\n",
        "sizes_over_50 = [percentage_over_50_stroke, 100 - percentage_over_50_stroke]\n",
        "fig, axs = plt.subplots(1, 2, figsize=(14, 7))\n",
        "\n",
        "# Pie chart for individuals under 50\n",
        "axs[0].pie(sizes_under_50, labels=labels, autopct='%1.1f%%', startangle=140, colors=['lightcoral', 'lightblue'])\n",
        "axs[0].set_title('Percentage of People Under 50 Having a Stroke')\n",
        "\n",
        "# Pie chart for individuals 50 and older\n",
        "axs[1].pie(sizes_over_50, labels=labels, autopct='%1.1f%%', startangle=140, colors=['lightcoral', 'lightblue'])\n",
        "axs[1].set_title('Percentage of People 50 and Older Having a Stroke')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0dfab72e",
      "metadata": {
        "id": "0dfab72e"
      },
      "source": [
        "## 3. Encoding our data for use in machine learning"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7e84729c",
      "metadata": {
        "id": "7e84729c"
      },
      "source": [
        "In machine learning, encoding data is essential for preparing categorical variables to be used as input in algorithms. Since most machine learning models require numerical data, categorical variables such as gender, smoking status, or work type need to be encoded into numerical form. This process ensures that the model can effectively interpret and learn from these features, enabling it to make accurate predictions or classifications based on the input data."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a0bc2d39",
      "metadata": {
        "id": "a0bc2d39"
      },
      "source": [
        "### Check data types, we will convert objects into categorical variables to be encoded"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4e1318fb",
      "metadata": {
        "id": "4e1318fb"
      },
      "outputs": [],
      "source": [
        "df.dtypes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1e7abd70",
      "metadata": {
        "id": "1e7abd70"
      },
      "outputs": [],
      "source": [
        "# Define categorical features for encoding\n",
        "catFeatures = ['gender','ever_married','work_type','Residence_type','smoking_status']\n",
        "# Describe the categorical features to see the number of unique categories in each\n",
        "df[catFeatures].describe(include='all').loc['unique', :]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "26c223ec",
      "metadata": {
        "id": "26c223ec"
      },
      "source": [
        "### Convert objects to categorical variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "10f5c0c0",
      "metadata": {
        "id": "10f5c0c0"
      },
      "outputs": [],
      "source": [
        "# Convert categorical columns to 'category' dtype for efficient encoding\n",
        "df[['gender','ever_married','work_type','Residence_type','smoking_status']] = df[['gender','ever_married','work_type','Residence_type','smoking_status']].astype('category')\n",
        "df.dtypes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a80bbf58",
      "metadata": {
        "id": "a80bbf58"
      },
      "outputs": [],
      "source": [
        "# Encode categorical features as integers\n",
        "for column in ['gender', 'ever_married', 'work_type', 'Residence_type', 'smoking_status']:\n",
        "    df[column] = df[column].astype('category').cat.codes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "695f60f6",
      "metadata": {
        "id": "695f60f6"
      },
      "outputs": [],
      "source": [
        "# Print the unique values in the encoded categorical columns for verification\n",
        "for column in ['gender', 'ever_married', 'work_type', 'Residence_type', 'smoking_status']:\n",
        "    unique_values = df[column].unique()\n",
        "    print(f\"Unique values in '{column}': {unique_values}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "102c1404",
      "metadata": {
        "id": "102c1404"
      },
      "source": [
        "### Check counts on gender, see if it is significant"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3ead0988",
      "metadata": {
        "id": "3ead0988"
      },
      "outputs": [],
      "source": [
        "# Check the distribution of values in the 'gender' column\n",
        "df['gender'].value_counts() # We'll treat it as a binary!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "35195d46",
      "metadata": {
        "id": "35195d46"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "273b6196",
      "metadata": {
        "id": "273b6196"
      },
      "source": [
        "### Create synthetic balance in the dataset using SMOTE"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a8754a7b",
      "metadata": {
        "id": "a8754a7b"
      },
      "source": [
        "Due to the imbalance in our dataset we utilize SMOTE and SMOTENC to create synthetic data to improve the outcomes of our machine learning models."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8e6bd7c8",
      "metadata": {
        "id": "8e6bd7c8"
      },
      "source": [
        "**You can choose either model, press 'ctrl + /' to uncomment or comment out code choice.** \\\n",
        "Rerun model with new choices for different outcomes"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fdf88839",
      "metadata": {
        "id": "fdf88839"
      },
      "source": [
        "## SMOTE\n",
        "We will use SMOTE and create synthetic data for both training and test."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "47e8754b",
      "metadata": {
        "id": "47e8754b"
      },
      "outputs": [],
      "source": [
        "oversampled = SMOTE()\n",
        "eval_df = df[['gender','age','hypertension','heart_disease','smoking_status','avg_glucose_level','bmi','stroke']].sample(int(df.shape[0]*0.2),random_state=42)\n",
        "train_df = df.drop(index=eval_df.index)\n",
        "\n",
        "X_test,y_test = eval_df[['gender','age','hypertension','heart_disease','smoking_status','avg_glucose_level','bmi']], eval_df['stroke']\n",
        "X_train,y_train = train_df[['gender','age','hypertension','heart_disease','smoking_status','avg_glucose_level','bmi']], train_df['stroke']\n",
        "\n",
        "\n",
        "X_train, y_train = oversampled.fit_resample(X_train,y_train)\n",
        "usampled_df = X_train.assign(Stroke = y_train)\n",
        "\n",
        "X_test,y_test = oversampled.fit_resample(X_test,y_test)\n",
        "usampled_eval_df = X_test.assign(Stroke = y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "21be21ee",
      "metadata": {
        "id": "21be21ee"
      },
      "source": [
        "## SMOTENC\n",
        "Another option is to use SMOTENC that creates only synthetic data for the training data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "09676dcc",
      "metadata": {
        "id": "09676dcc"
      },
      "outputs": [],
      "source": [
        "# # Run train test split\n",
        "# X = df.drop(['stroke'], axis=1)\n",
        "# y = df['stroke']\n",
        "\n",
        "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=85)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "909a6ef8",
      "metadata": {
        "id": "909a6ef8"
      },
      "outputs": [],
      "source": [
        "# oversample = SMOTENC(categorical_features=[0,2,3,4,5,6,9],\n",
        "#                     random_state=27,  # for reproducibility\n",
        "#                     sampling_strategy='auto')\n",
        "\n",
        "# X_train, y_train = oversample.fit_resample(X_train, y_train)\n",
        "# X_test, y_test = oversample.fit_resample(X_test, y_test)\n",
        "\n",
        "# print('Original class distribution: \\n')\n",
        "# print(y_train.value_counts())\n",
        "# print('-----------------------------------------')\n",
        "# print('Synthetic sample class distribution: \\n')\n",
        "# print(pd.Series(y_train_res).value_counts())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "81eb8094",
      "metadata": {
        "id": "81eb8094"
      },
      "source": [
        "## 4. Choose scaling method"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3d912828",
      "metadata": {
        "id": "3d912828"
      },
      "source": [
        "**You can choose either model, press 'ctrl + /' to uncomment or comment out code choice.**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cbdf6f63",
      "metadata": {
        "id": "cbdf6f63"
      },
      "source": [
        " <u>Normalization<u/> rescales the features to a fixed range, usually 0 to 1.\n",
        "\n",
        "Advantages:\n",
        "\n",
        " - Useful when you need to bound your values between a specific range.\n",
        " - Maintains the original distribution without distorting differences in the ranges of values.\n",
        "\n",
        "Disadvantages:\n",
        "\n",
        " - If your data contains outliers, normalization can squash the \"normal\" data into a small portion of the range, reducing the      algorithm's ability to learn from it."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3a5db4a3",
      "metadata": {
        "id": "3a5db4a3"
      },
      "source": [
        "<u>Standardization<u/> rescales data so that it has a mean of 0 and a standard deviation of 1.\n",
        "\n",
        "Advantages:\n",
        "\n",
        " - Standardization does not bound values to a specific range, which might be useful for certain algorithms that assume no specific range.\n",
        " - More robust to outliers compared to normalization.\n",
        "\n",
        "Disadvantages:\n",
        "\n",
        " - The resulting distribution will have a mean of 0 and a standard deviation of 1, but it might not be suitable for algorithms that expect input data to be within a bounded range."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e6f6ce1b",
      "metadata": {
        "id": "e6f6ce1b"
      },
      "source": [
        "### Normalization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "961cf110",
      "metadata": {
        "id": "961cf110"
      },
      "outputs": [],
      "source": [
        "# from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# # Selecting numerical columns that need normalization\n",
        "# numerical_cols = ['age', 'avg_glucose_level', 'work_type', 'bmi', 'smoking_status']\n",
        "\n",
        "# # Initialize the MinMaxScaler\n",
        "# scaler = MinMaxScaler()\n",
        "\n",
        "# # Fit on training data\n",
        "# scaler.fit(X_train[numerical_cols])\n",
        "\n",
        "# # Transform both training and testing data\n",
        "# X_train[numerical_cols] = scaler.transform(X_train[numerical_cols])\n",
        "# X_test[numerical_cols] = scaler.transform(X_test[numerical_cols])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "82ccf861",
      "metadata": {
        "id": "82ccf861"
      },
      "source": [
        "### Standardization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "36d3dc6e",
      "metadata": {
        "id": "36d3dc6e"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Selecting numerical columns that need normalization\n",
        "numerical_cols = ['age', 'avg_glucose_level', 'smoking_status', 'bmi']\n",
        "\n",
        "# Initialize the StandardScaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Fit on training data\n",
        "scaler.fit(X_train[numerical_cols])\n",
        "\n",
        "# Transform both training and testing data\n",
        "X_train[numerical_cols] = scaler.transform(X_train[numerical_cols])\n",
        "X_test[numerical_cols] = scaler.transform(X_test[numerical_cols])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ad5924c",
      "metadata": {
        "id": "4ad5924c"
      },
      "outputs": [],
      "source": [
        "# Verify processing worked\n",
        "X_train.head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "be3572be",
      "metadata": {
        "scrolled": true,
        "id": "be3572be"
      },
      "outputs": [],
      "source": [
        "# Verify processing worked\n",
        "X_test.head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3ff81cf0",
      "metadata": {
        "id": "3ff81cf0"
      },
      "outputs": [],
      "source": [
        "display(X_train.shape)\n",
        "display(X_train.info())\n",
        "display(X_train.describe())\n",
        "display(X_train.columns)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c7272950",
      "metadata": {
        "id": "c7272950"
      },
      "source": [
        "## 5. Decision Tree"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cb3e0b41",
      "metadata": {
        "id": "cb3e0b41"
      },
      "source": [
        "A decision tree is a hierarchical model that helps in making decisions by mapping out possible outcomes based on different conditions. It's a visual representation where each branch represents a decision based on features in the data, ultimately leading to a prediction or classification."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "111fac3b",
      "metadata": {
        "id": "111fac3b"
      },
      "outputs": [],
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "model_dt = DecisionTreeClassifier()\n",
        "model_dt.fit(X_train,y_train)\n",
        "y_pred = model_dt.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "560ee571",
      "metadata": {
        "id": "560ee571"
      },
      "outputs": [],
      "source": [
        "y_pred_train = model_dt.predict(X_train)\n",
        "y_pred_test = model_dt.predict(X_test)\n",
        "print(y_pred_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fb95b357",
      "metadata": {
        "id": "fb95b357"
      },
      "outputs": [],
      "source": [
        "print('Classification Report for Testing:')\n",
        "print(classification_report(y_test, y_pred_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0dbe3a4b",
      "metadata": {
        "id": "0dbe3a4b"
      },
      "outputs": [],
      "source": [
        "dt_bas = round(balanced_accuracy_score(y_test, y_pred),2)\n",
        "print(f'Decision Tree balanced accuracy score {dt_bas}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0572bed8",
      "metadata": {
        "id": "0572bed8"
      },
      "outputs": [],
      "source": [
        "# Place scores in dictionar\n",
        "metrics_test = precision_recall_fscore_support(y_test, y_pred, average='binary')\n",
        "dt_results = {\n",
        "    'Method': 'Decision Tree',\n",
        "    'Precision': round(metrics_test[0],2),\n",
        "    'Recall': round(metrics_test[1],2),\n",
        "    'F1 Score': round(metrics_test[2],2),\n",
        "    'Balanced Accuracy': dt_bas\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9216056d",
      "metadata": {
        "scrolled": true,
        "id": "9216056d"
      },
      "outputs": [],
      "source": [
        "dt_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "75564ddc",
      "metadata": {
        "id": "75564ddc"
      },
      "outputs": [],
      "source": [
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "plt.figure(figsize=(6, 6))\n",
        "sns.heatmap(cm, annot=True, cmap='Blues', fmt='g')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c34a0d9c",
      "metadata": {
        "id": "c34a0d9c"
      },
      "source": [
        "### We will find a good max_depth to run with our model to see if we can improve"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ddb4daf3",
      "metadata": {
        "id": "ddb4daf3"
      },
      "outputs": [],
      "source": [
        "models = {'train_score': [], 'test_score': [], 'max_depth': []}\n",
        "\n",
        "for depth in range(1,15):\n",
        "    models['max_depth'].append(depth)\n",
        "    model = DecisionTreeClassifier( max_depth=depth)\n",
        "    model.fit(X_train, y_train)\n",
        "    y_test_pred = model.predict(X_test)\n",
        "    y_train_pred = model.predict(X_train)\n",
        "\n",
        "    models['train_score'].append(balanced_accuracy_score(y_train, y_train_pred))\n",
        "    models['test_score'].append(balanced_accuracy_score(y_test, y_test_pred))\n",
        "\n",
        "models_df = pd.DataFrame(models)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f76031a3",
      "metadata": {
        "id": "f76031a3"
      },
      "outputs": [],
      "source": [
        "models_df.plot(x='max_depth')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "07deeae7",
      "metadata": {
        "id": "07deeae7"
      },
      "source": [
        "You want to pick the max_depth where the test_score peaks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "583e7da9",
      "metadata": {
        "scrolled": true,
        "id": "583e7da9"
      },
      "outputs": [],
      "source": [
        "model = DecisionTreeClassifier(max_depth=7, random_state=42) # Insert max_depth from above graph\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "dt_md_bas = round(balanced_accuracy_score(y_test, y_pred),2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "29cc725b",
      "metadata": {
        "scrolled": true,
        "id": "29cc725b"
      },
      "outputs": [],
      "source": [
        "print(f'Random Forest with adjusted max_depth balanced accuracy score: {dt_md_bas}') # The tuning should increase the score!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "217e6f5d",
      "metadata": {
        "id": "217e6f5d"
      },
      "outputs": [],
      "source": [
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Displaying the confusion matrix as a heatmap using Seaborn\n",
        "plt.figure(figsize=(6, 6))\n",
        "sns.heatmap(cm, annot=True, cmap='Blues', fmt='g')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "12baf658",
      "metadata": {
        "id": "12baf658"
      },
      "outputs": [],
      "source": [
        "print('Classification Report for Testing:')\n",
        "print(classification_report(y_test, y_pred_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5e46a6f9",
      "metadata": {
        "id": "5e46a6f9"
      },
      "outputs": [],
      "source": [
        "# Place scores in dictionary\n",
        "metrics_test = precision_recall_fscore_support(y_test, y_pred, average='binary')\n",
        "dt_md_results = {\n",
        "    'Method': 'Decision Tree max_depth',\n",
        "    'Precision': round(metrics_test[0],2),\n",
        "    'Recall': round(metrics_test[1],2),\n",
        "    'F1 Score': round(metrics_test[2],2),\n",
        "    'Balanced Accuracy': dt_md_bas\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1c1e6302",
      "metadata": {
        "id": "1c1e6302"
      },
      "outputs": [],
      "source": [
        "dt_md_results"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0f1598bf",
      "metadata": {
        "id": "0f1598bf"
      },
      "source": [
        "### 5.5 PCA"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d9dba89b",
      "metadata": {
        "id": "d9dba89b"
      },
      "source": [
        "Applying PCA before using a Random Forest classifier can help reduce dimensionality and computational costs, potentially improve model generalization by removing noise, but it may obscure the interpretability of feature importance and, depending on the dataset, could either improve or degrade performance. We are choosing to run it here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8b0ae6ba",
      "metadata": {
        "id": "8b0ae6ba"
      },
      "outputs": [],
      "source": [
        "pca_model = PCA(n_components = 7) # 7 for SMOTE, 10 for SMOTENC\n",
        "pca_model.fit(X_train)\n",
        "\n",
        "X_train_pca = pd.DataFrame(pca_model.transform(X_train))\n",
        "X_test_pca = pd.DataFrame(pca_model.transform(X_test))\n",
        "X_train_pca"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9cc0df9d",
      "metadata": {
        "id": "9cc0df9d"
      },
      "source": [
        "### 6. Random Forest"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "be870e3c",
      "metadata": {
        "id": "be870e3c"
      },
      "source": [
        "A Random Forest is a machine learning method used in both classification and regression tasks. It operates by constructing a multitude of decision trees during training time and outputs the mode or average prediction of the individual trees."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa420877",
      "metadata": {
        "id": "aa420877"
      },
      "outputs": [],
      "source": [
        "model = RandomForestClassifier(n_estimators=100, class_weight='balanced', random_state=42)\n",
        "model.fit(X_train_pca, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e7d8e4a0",
      "metadata": {
        "id": "e7d8e4a0"
      },
      "outputs": [],
      "source": [
        "y_test_pred = model.predict(X_test_pca)\n",
        "rf_bas = round(balanced_accuracy_score(y_test, y_test_pred),2)\n",
        "print(f'Random Forest balanced accuracy score: {rf_bas}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d8b5e796",
      "metadata": {
        "id": "d8b5e796"
      },
      "outputs": [],
      "source": [
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Displaying the confusion matrix as a heatmap using Seaborn\n",
        "plt.figure(figsize=(6, 6))\n",
        "sns.heatmap(cm, annot=True, cmap='Blues', fmt='g')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e59b1879",
      "metadata": {
        "id": "e59b1879"
      },
      "outputs": [],
      "source": [
        "print('Classification Report for Testing:')\n",
        "print(classification_report(y_test, y_pred_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2c26db15",
      "metadata": {
        "id": "2c26db15"
      },
      "outputs": [],
      "source": [
        "# Place scores in dictionary\n",
        "metrics_test = precision_recall_fscore_support(y_test, y_pred, average='binary')\n",
        "rf_results = {\n",
        "    'Method': 'Random Forest',\n",
        "    'Precision': round(metrics_test[0],2),\n",
        "    'Recall': round(metrics_test[1],2),\n",
        "    'F1 Score': round(metrics_test[2],2),\n",
        "    'Balanced Accuracy': rf_bas\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "db743891",
      "metadata": {
        "id": "db743891"
      },
      "outputs": [],
      "source": [
        "rf_results"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "000db4c0",
      "metadata": {
        "id": "000db4c0"
      },
      "source": [
        "### We will find a good max_depth to run with our model to see if we can improve"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7575d4b9",
      "metadata": {
        "id": "7575d4b9"
      },
      "outputs": [],
      "source": [
        "models = {'train_score': [], 'test_score': [], 'max_depth': []}\n",
        "\n",
        "for depth in range(1,10):\n",
        "    models['max_depth'].append(depth)\n",
        "    model = RandomForestClassifier(n_estimators=100, max_depth=depth)\n",
        "    model.fit(X_train_pca, y_train)\n",
        "    y_test_pred = model.predict(X_test_pca)\n",
        "    y_train_pred = model.predict(X_train_pca)\n",
        "\n",
        "    models['train_score'].append(balanced_accuracy_score(y_train, y_train_pred))\n",
        "    models['test_score'].append(balanced_accuracy_score(y_test, y_test_pred))\n",
        "\n",
        "models_df = pd.DataFrame(models)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bc5d14ea",
      "metadata": {
        "scrolled": true,
        "id": "bc5d14ea"
      },
      "outputs": [],
      "source": [
        "models_df.plot(x='max_depth')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "50b5e3ef",
      "metadata": {
        "id": "50b5e3ef"
      },
      "source": [
        "You want to pick the max_depth where the test_score peaks."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "caa2848c",
      "metadata": {
        "id": "caa2848c"
      },
      "source": [
        "### Apply best max_depth to Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b3a2b039",
      "metadata": {
        "id": "b3a2b039"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "model = RandomForestClassifier(n_estimators=100, max_depth=7, random_state=42) # Insert max_depth from above graph\n",
        "model.fit(X_train, y_train)\n",
        "forest_score = model.score(X_train, y_train)\n",
        "forest_test = model.score(X_test, y_test)\n",
        "y_pred = model.predict(X_test)\n",
        "rf_md_bas = round(balanced_accuracy_score(y_test, y_pred),2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4f4c91ef",
      "metadata": {
        "id": "4f4c91ef"
      },
      "outputs": [],
      "source": [
        "print(f'Random Forest with adjusted max_depth balanced accuracy score: {rf_md_bas}') # The tuning should increase the score!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f2f2de91",
      "metadata": {
        "id": "f2f2de91"
      },
      "outputs": [],
      "source": [
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Displaying the confusion matrix as a heatmap using Seaborn\n",
        "plt.figure(figsize=(6, 6))\n",
        "sns.heatmap(cm, annot=True, cmap='Blues', fmt='g')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "00f00d23",
      "metadata": {
        "id": "00f00d23"
      },
      "outputs": [],
      "source": [
        "print('Classification Report for Testing:')\n",
        "print(classification_report(y_test, y_pred_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5c91bc8e",
      "metadata": {
        "id": "5c91bc8e"
      },
      "outputs": [],
      "source": [
        "# Place scores in dictionary\n",
        "metrics_test = precision_recall_fscore_support(y_test, y_pred, average='binary')\n",
        "rf_md_results = {\n",
        "    'Method': 'Random Forest max_depth',\n",
        "    'Precision': round(metrics_test[0],2),\n",
        "    'Recall': round(metrics_test[1],2),\n",
        "    'F1 Score': round(metrics_test[2],2),\n",
        "    'Balanced Accuracy': rf_md_bas\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "90992694",
      "metadata": {
        "id": "90992694"
      },
      "outputs": [],
      "source": [
        "rf_md_results"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e66f1a39",
      "metadata": {
        "id": "e66f1a39"
      },
      "source": [
        "## 7. K Nearest Neighbors"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cdec5693",
      "metadata": {
        "id": "cdec5693"
      },
      "source": [
        "The k-nearest neighbors algorithm predicts the label of a data point based on the labels of its 'k' closest neighbors in the dataset. To classify a new instance, KNN calculates the distance between the instance and all points in the training set, identifies the 'k' nearest points, and then uses a majority vote among these neighbors to determine the instance's label. For regression tasks, it averages the values of these neighbors instead."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4938f3d9",
      "metadata": {
        "id": "4938f3d9"
      },
      "outputs": [],
      "source": [
        "param_grid = {\n",
        "    'n_neighbors': [1, 3, 5, 7, 9, 11, 13, 15, 17, 19],\n",
        "    'weights': ['uniform', 'distance'],\n",
        "    'leaf_size': [10, 50, 100, 500]\n",
        "}\n",
        "random_knn = RandomizedSearchCV(KNeighborsClassifier(), param_grid, verbose=3)\n",
        "\n",
        "random_knn.fit(X_train_pca, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c23c85b",
      "metadata": {
        "id": "4c23c85b"
      },
      "outputs": [],
      "source": [
        "y_pred = random_knn.predict(X_test_pca)\n",
        "knn_bas = round(balanced_accuracy_score(y_test, y_pred),2)\n",
        "print(f'KNN balanced accuracy score: {knn_bas}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dc28c9ea",
      "metadata": {
        "id": "dc28c9ea"
      },
      "outputs": [],
      "source": [
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Displaying the confusion matrix as a heatmap using Seaborn\n",
        "plt.figure(figsize=(6, 6))\n",
        "sns.heatmap(cm, annot=True, cmap='Blues', fmt='g')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2d2774af",
      "metadata": {
        "id": "2d2774af"
      },
      "outputs": [],
      "source": [
        "print('Classification Report for Testing:')\n",
        "print(classification_report(y_test, y_pred_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8a7656aa",
      "metadata": {
        "id": "8a7656aa"
      },
      "outputs": [],
      "source": [
        "# Place scores in dictionary\n",
        "metrics_test = precision_recall_fscore_support(y_test, y_pred, average='binary')\n",
        "knn_results = {\n",
        "    'Method': 'KNN max_depth',\n",
        "    'Precision': round(metrics_test[0],2),\n",
        "    'Recall': round(metrics_test[1],2),\n",
        "    'F1 Score': round(metrics_test[2],2),\n",
        "    'Balanced Accuracy': knn_bas\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "85e22335",
      "metadata": {
        "id": "85e22335"
      },
      "outputs": [],
      "source": [
        "knn_results"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "47d237b5",
      "metadata": {
        "id": "47d237b5"
      },
      "source": [
        "### Let's tune our KNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fa2b6c79",
      "metadata": {
        "id": "fa2b6c79"
      },
      "outputs": [],
      "source": [
        "# Define ranges and settings to explore\n",
        "n_neighbors_range = range(1, 50)\n",
        "weights_options = ['uniform', 'distance']\n",
        "scores = {weight: [] for weight in weights_options}\n",
        "\n",
        "for weight in weights_options:\n",
        "    for n_neighbors in n_neighbors_range:\n",
        "        knn = KNeighborsClassifier(n_neighbors=n_neighbors, weights=weight)\n",
        "        knn.fit(X_train_pca, y_train)\n",
        "        y_pred = knn.predict(X_test_pca)\n",
        "        score = balanced_accuracy_score(y_test, y_pred)\n",
        "        scores[weight].append(score)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e4c1eed1",
      "metadata": {
        "id": "e4c1eed1"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12, 6))\n",
        "for weight in weights_options:\n",
        "    plt.plot(n_neighbors_range, scores[weight], label=f'Weights: {weight}')\n",
        "plt.xlabel('Number of Neighbors')\n",
        "plt.ylabel('Balanced Accuracy Score')\n",
        "plt.legend()\n",
        "plt.title('KNN Performance: n_neighbors vs. Balanced Accuracy')\n",
        "plt.xticks(list(n_neighbors_range))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6a5adcc8",
      "metadata": {
        "id": "6a5adcc8"
      },
      "source": [
        "Select the either 'uniform' or 'distance' line which has the highest peak for optimal_weights, select corresponding number of neighbors for optimal_n_neighbors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb0456fd",
      "metadata": {
        "id": "bb0456fd"
      },
      "outputs": [],
      "source": [
        "optimal_n_neighbors = 44 # Select fro above graph\n",
        "optimal_weights = 'distance'\n",
        "\n",
        "optimal_knn = KNeighborsClassifier(n_neighbors=optimal_n_neighbors, weights=optimal_weights)\n",
        "optimal_knn.fit(X_train, y_train)\n",
        "y_pred_optimal = optimal_knn.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3bbe5042",
      "metadata": {
        "id": "3bbe5042"
      },
      "outputs": [],
      "source": [
        "knn_md_bas = round(balanced_accuracy_score(y_test, y_pred_optimal),2)\n",
        "print(f'KNN with adjusted neighbors accuracy score: {knn_md_bas}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f1b6dda0",
      "metadata": {
        "id": "f1b6dda0"
      },
      "outputs": [],
      "source": [
        "cm = confusion_matrix(y_test, y_pred_optimal)\n",
        "\n",
        "# Displaying the confusion matrix as a heatmap using Seaborn\n",
        "plt.figure(figsize=(6, 6))\n",
        "sns.heatmap(cm, annot=True, cmap='Blues', fmt='g')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "074ea295",
      "metadata": {
        "id": "074ea295"
      },
      "outputs": [],
      "source": [
        "print('Classification Report for Testing:')\n",
        "print(classification_report(y_test, y_pred_optimal))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "19270e76",
      "metadata": {
        "id": "19270e76"
      },
      "outputs": [],
      "source": [
        "# Place scores in dictionary\n",
        "metrics_test = precision_recall_fscore_support(y_test, y_pred_optimal, average='binary')\n",
        "knn_md_results = {\n",
        "    'Method': 'KNN modified neighbors',\n",
        "    'Precision': round(metrics_test[0],2),\n",
        "    'Recall': round(metrics_test[1],2),\n",
        "    'F1 Score': round(metrics_test[2],2),\n",
        "    'Balanced Accuracy': knn_md_bas\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "605a8808",
      "metadata": {
        "scrolled": true,
        "id": "605a8808"
      },
      "outputs": [],
      "source": [
        "knn_md_results"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d1eb144f",
      "metadata": {
        "id": "d1eb144f"
      },
      "source": [
        "## Conclusion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "979d34b4",
      "metadata": {
        "id": "979d34b4"
      },
      "outputs": [],
      "source": [
        "all_results = []\n",
        "all_results.append(dt_results)\n",
        "all_results.append(dt_md_results)\n",
        "all_results.append(rf_results)\n",
        "all_results.append(rf_md_results)\n",
        "all_results.append(knn_results)\n",
        "all_results.append(knn_md_results)\n",
        "\n",
        "df_results = pd.DataFrame(all_results)\n",
        "df_results.set_index('Method', inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1777c75f",
      "metadata": {
        "id": "1777c75f"
      },
      "outputs": [],
      "source": [
        "ax = df_results.plot(kind='bar', figsize=(10, 6), width=0.8)\n",
        "plt.title('Model Performance Comparison')\n",
        "plt.ylabel('Score')\n",
        "plt.xticks(rotation=45)\n",
        "plt.grid(axis='y', linestyle='--', linewidth=0.7)\n",
        "\n",
        "for p in ax.patches:\n",
        "    ax.annotate(str(round(p.get_height(), 2)), (p.get_x() * 1.005, p.get_height() * 1.005), fontsize=9)\n",
        "\n",
        "plt.legend(title='Method', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7616554f",
      "metadata": {
        "id": "7616554f"
      },
      "outputs": [],
      "source": [
        "metrics = ['Precision', 'F1 Score', 'Recall', 'Balanced Accuracy']\n",
        "for metric in metrics:\n",
        "    max_value = df_results[metric].max()\n",
        "    max_model = df_results[df_results[metric] == max_value].index[0]\n",
        "    print(f\"Model with highest {metric}: {max_model} ({max_value})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aedcc48c",
      "metadata": {
        "id": "aedcc48c"
      },
      "source": [
        "The initial analysis of the dataset revealed a significant imbalance, raising concerns about data leakage potential. To mitigate this, we experimented with both SMOTE and SMOTENC for oversampling, with SMOTE demonstrating greater performance in addressing the imbalance.\n",
        "\n",
        "Upon evaluating various machine learning models for classification purposes, it was observed that prior to tuning the models did not exhibit strong predictive capabilities. However post-tuning improvements were notable, particularly in terms of balanced accuracy scores. Examining other projects that used our dataset had similar findings. An interesting discovery during our investigation was that datasets incorporating bloodwork data tend to yield more accurate stroke predictions. This suggests that lifestyle-based predictive models might best serve as preliminary tools for healthcare professionals, guiding at-risk patients towards more definitive bloodwork analyses.\n",
        "\n",
        "Despite the challenges presented by lifestyle data, the Random Forest Classifier was the standout model upon tuning, specifically when adjusted to the optimal max depth. This model achieved a balanced accuracy score of 80%, marking it as the most effective among the classifiers we tested for predicting stroke potential. The Random Forest Classifier with an appropriate max depth is what we would recommended as a tool for stroke prediction, emphasizing the model's utility in clinical settings for early stroke risk assessment."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "LodYSZT7NyEf"
      },
      "id": "LodYSZT7NyEf"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Deep Learning & interface\n",
        "At this point in the project we can begin implementing a deep learning model for stroke prediction. We could potentially improve the performance of our predictions. Deep learning is particularly good at capturing non-linear interactions between features. Here’s how we will approach the next part of this project:\n",
        "\n",
        "##Deep Learning Model Approach\n",
        "###Data Preprocessing:\n",
        "1. Normalize or standardize the input features to ensure that the model trains efficiently using One-hot encode categorical variables\n",
        "\n",
        "###Model Architecture:\n",
        "1. Use a simple feedforward neural network with several dense layers as a starting point.\n",
        "2. Include dropout layers to prevent overfitting.\n",
        "3. Use activation function ReLU for hidden layers and a sigmoid activation function at the output layer for binary classification (stroke or no stroke).\n",
        "\n",
        "###Compilation:\n",
        "1. Compile the model using the optimizer Adam.\n",
        "2. Use binary cross-entropy as the loss function since this is a binary classification problem.\n",
        "3. Track accuracy as a metric and hypertune our model using Optuna\n",
        "\n",
        "###Training:\n",
        "1. Train the model using a suitable batch size and number of epochs.\n",
        "2. Utilize callbacks like ModelCheckpoint for saving the best model and EarlyStopping to halt training when performance plateaus, to overcome overfitting we will impliment a ReduceLR into our epochs\n",
        "\n",
        "###Evaluation:\n",
        "1. Evaluate the model on a validation set to check for overfitting and underfitting.\n",
        "2. Adjust the model architecture and hyperparameters based on performance metrics.\n",
        "\n",
        "###Deployment:\n",
        "1. Once the model is trained and validated, deploy it in a Gradio interface to make it interactive."
      ],
      "metadata": {
        "id": "exF_PSKGN1-8"
      },
      "id": "exF_PSKGN1-8"
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Extra Analysis\n",
        "##Preprocessing\n",
        "###OHE\n",
        "###TT&S\n",
        "###Smote\n",
        "###Val"
      ],
      "metadata": {
        "id": "3E9inah8UDEI"
      },
      "id": "3E9inah8UDEI"
    },
    {
      "cell_type": "code",
      "source": [
        "df.drop('age_group', axis=1, inplace=True)\n"
      ],
      "metadata": {
        "id": "pJgDcuvZ2rON"
      },
      "id": "pJgDcuvZ2rON",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the original dataset with continuous age values\n",
        "df_continuous = df.copy()\n",
        "\n",
        "# Scatter Plot for Age vs. Avg Glucose Level colored by Stroke outcome\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.scatterplot(x='age', y='avg_glucose_level', hue='stroke', data=df_continuous)\n",
        "plt.title('Age vs. Avg Glucose Level by Stroke Outcome')\n",
        "plt.show()\n",
        "\n",
        "# Line Plot for Age vs. Stroke Rate\n",
        "# Calculate stroke rate by age\n",
        "age_stroke_rate = df_continuous.groupby('age')['stroke'].mean().reset_index()\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.lineplot(x='age', y='stroke', data=age_stroke_rate)\n",
        "plt.title('Stroke Rate by Age')\n",
        "plt.xlabel('Age')\n",
        "plt.ylabel('Stroke Rate')\n",
        "plt.show()\n",
        "\n",
        "# Pairplot for Age, BMI, Glucose and Stroke\n",
        "sns.pairplot(df_continuous, vars=['age', 'bmi', 'avg_glucose_level'], hue='stroke')\n",
        "plt.suptitle('Pairwise Relationships for Age, BMI, Glucose Level')\n",
        "plt.show()\n",
        "\n",
        "# Heatmap for Age and Stroke Correlation with Other Variables\n",
        "corr_matrix = df_continuous.corr()\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')\n",
        "plt.title('Correlation Heatmap')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "jHg3lYt0APSn"
      },
      "id": "jHg3lYt0APSn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install catboost\n"
      ],
      "metadata": {
        "id": "3xaKcLD44AWP"
      },
      "id": "3xaKcLD44AWP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install optuna\n"
      ],
      "metadata": {
        "id": "i02-FX0-DtWj"
      },
      "id": "i02-FX0-DtWj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "I3guQvnTFckB"
      },
      "id": "I3guQvnTFckB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_recall_curve\n",
        "import numpy as np\n",
        "\n",
        "# Predict probabilities\n",
        "probabilities = model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Calculate precision-recall pairs for different probability thresholds\n",
        "precisions, recalls, thresholds = precision_recall_curve(y_test, probabilities)\n",
        "\n",
        "# Find the threshold that gives the best precision while maintaining reasonable recall\n",
        "threshold_index = np.argmax(precisions >= 0.95)  # Change 0.95 to the desired precision level\n",
        "best_threshold = thresholds[threshold_index]\n",
        "print(\"Best threshold for high precision:\", best_threshold)\n"
      ],
      "metadata": {
        "id": "05xfaH1NlqAS"
      },
      "id": "05xfaH1NlqAS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply threshold to positive probabilities to create binary output\n",
        "predictions = (probabilities >= best_threshold).astype(int)\n",
        "\n",
        "# Evaluate the final model precision and other metrics\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "print(classification_report(y_test, predictions))\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, predictions))\n"
      ],
      "metadata": {
        "id": "89oG64LxlqEX"
      },
      "id": "89oG64LxlqEX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Bin 'BMI' into categories\n",
        "df['bmi_bins'] = pd.cut(df['bmi'], bins=[0, 18.5, 24.9, 29.9, 34.9, 39.9, np.inf], labels=[0, 1, 2, 3, 4, 5])\n",
        "\n",
        "# Bin 'avg_glucose_level' into categories based on common medical knowledge or quartiles\n",
        "df['glucose_bins'] = pd.cut(df['avg_glucose_level'], bins=[0, 90, 140, 200, np.inf], labels=[0, 1, 2, 3])\n",
        "\n"
      ],
      "metadata": {
        "id": "tgYwryhamrPI"
      },
      "id": "tgYwryhamrPI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GvhQfTVPnZnw"
      },
      "id": "GvhQfTVPnZnw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8c11VqHAnZtR"
      },
      "id": "8c11VqHAnZtR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "xhADQSRenJ8o"
      },
      "id": "xhADQSRenJ8o",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.drop(['bmi', 'avg_glucose_level'], axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "4GdMR9T9oQJg"
      },
      "id": "4GdMR9T9oQJg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.dtypes)\n"
      ],
      "metadata": {
        "id": "8uA6vu6ktB5q"
      },
      "id": "8uA6vu6ktB5q",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "import pandas as pd\n",
        "\n",
        "# Convert bin columns to type 'category' if not already\n",
        "df[['bmi_bins', 'glucose_bins']] = df[['bmi_bins', 'glucose_bins']].astype('category')\n",
        "\n",
        "# Convert category columns to integers\n",
        "df['bmi_bins'] = df['bmi_bins'].cat.codes\n",
        "df['glucose_bins'] = df['glucose_bins'].cat.codes\n",
        "\n",
        "# Set stroke as the target\n",
        "X = df.drop('stroke', axis=1)  # Features\n",
        "y = df['stroke']  # Target\n",
        "\n",
        "# Apply one-hot encoding to X\n",
        "X = pd.get_dummies(X)\n",
        "\n",
        "X.head()"
      ],
      "metadata": {
        "id": "yEgPV2YmUwk8"
      },
      "id": "yEgPV2YmUwk8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Scale 'age' feature\n",
        "scaler = MinMaxScaler()\n",
        "X['age'] = scaler.fit_transform(X[['age']])"
      ],
      "metadata": {
        "id": "Pz6lCnIw6dTz"
      },
      "id": "Pz6lCnIw6dTz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n"
      ],
      "metadata": {
        "id": "hdj818sSU2Eg"
      },
      "id": "hdj818sSU2Eg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "smote = SMOTE(random_state=42)\n",
        "X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n"
      ],
      "metadata": {
        "id": "FXJQbrvmVXGI"
      },
      "id": "FXJQbrvmVXGI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_smote, X_val, y_train_smote, y_val = train_test_split(X_train_smote, y_train_smote, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "12GaqfryVXpN"
      },
      "id": "12GaqfryVXpN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from catboost import CatBoostClassifier\n",
        "\n",
        "# Initialize the CatBoostClassifier\n",
        "model = CatBoostClassifier(\n",
        "    iterations=100,\n",
        "    depth=10,\n",
        "    learning_rate=0.05,\n",
        "    random_strength=2,\n",
        "    bagging_temperature=0.2,\n",
        "    od_type='IncToDec',\n",
        "    l2_leaf_reg=3,\n",
        "    loss_function='Logloss',\n",
        "    eval_metric='Precision',  # Focus on Precision during training\n",
        "    verbose=False\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train_smote, y_train_smote, eval_set=(X_val, y_val))\n"
      ],
      "metadata": {
        "id": "KEhGriqCnZjB"
      },
      "id": "KEhGriqCnZjB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "# Random Forest model\n",
        "rf_model = RandomForestClassifier(max_depth=7, random_state=42)\n",
        "rf_model.fit(X_train_smote, y_train_smote)\n",
        "y_pred_rf = rf_model.predict(X_test)\n",
        "\n",
        "print(\"Random Forest Performance:\")\n",
        "print(classification_report(y_test, y_pred_rf))\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_rf))\n"
      ],
      "metadata": {
        "id": "Wl4Z3qmYVXtU"
      },
      "id": "Wl4Z3qmYVXtU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Decision Tree model\n",
        "dt_model = DecisionTreeClassifier(max_depth=7, random_state=42)\n",
        "dt_model.fit(X_train_smote, y_train_smote)\n",
        "y_pred_dt = dt_model.predict(X_test)\n",
        "\n",
        "print(\"Decision Tree Performance:\")\n",
        "print(classification_report(y_test, y_pred_dt))\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_dt))\n",
        "\n"
      ],
      "metadata": {
        "id": "pVWwbd3E56Pa"
      },
      "id": "pVWwbd3E56Pa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import VotingClassifier\n",
        "\n",
        "# Create a voting classifier that includes Random Forest and Decision Tree\n",
        "ensemble_model = VotingClassifier(estimators=[\n",
        "    ('rf', rf_model),\n",
        "    ('dt', dt_model)\n",
        "], voting='hard')\n",
        "\n",
        "ensemble_model.fit(X_train_smote, y_train_smote)\n",
        "y_pred_ensemble = ensemble_model.predict(X_test)\n",
        "\n",
        "print(\"Ensemble Model Performance:\")\n",
        "print(classification_report(y_test, y_pred_ensemble))\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_ensemble))\n",
        "\n"
      ],
      "metadata": {
        "id": "wxxSj4cy5-vy"
      },
      "id": "wxxSj4cy5-vy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Extracting feature importance from the Random Forest model\n",
        "feature_importances = rf_model.feature_importances_\n",
        "features = X_train.columns\n",
        "importance_df = pd.DataFrame({'Features': features, 'Importance': feature_importances}).sort_values(by='Importance', ascending=False)\n",
        "\n",
        "# Plotting feature importances\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.barh(importance_df['Features'], importance_df['Importance'])\n",
        "plt.xlabel('Importance')\n",
        "plt.ylabel('Features')\n",
        "plt.title('Feature Importance from Random Forest')\n",
        "plt.gca().invert_yaxis()  # Invert y-axis to show the most important at the top\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "rCY4F2HC6CW5"
      },
      "id": "rCY4F2HC6CW5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_smote.head()"
      ],
      "metadata": {
        "id": "VtdQPrHLpVsO"
      },
      "id": "VtdQPrHLpVsO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report, roc_auc_score\n",
        "\n",
        "# Calculate the ratio for the positive class weight\n",
        "ratio = float(np.sum(y_train_smote == 0)) / np.sum(y_train_smote == 1)\n",
        "\n",
        "# Instantiate an XGBClassifier with scale_pos_weight\n",
        "xgb_clf = XGBClassifier(\n",
        "    n_estimators=100,\n",
        "    learning_rate=0.1,\n",
        "    max_depth=7,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8,\n",
        "    gamma=1,\n",
        "    scale_pos_weight=ratio,  # Setting the class weight\n",
        "    use_label_encoder=False,\n",
        "    eval_metric='logloss',\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Fit the classifier to the training data\n",
        "xgb_clf.fit(X_train_smote, y_train_smote)\n",
        "\n",
        "# Predict the labels for the test set\n",
        "y_pred_xgb = xgb_clf.predict(X_test)\n",
        "\n",
        "# Evaluate the classifier\n",
        "print(\"Adjusted XGBoost Performance:\")\n",
        "print(classification_report(y_test, y_pred_xgb))\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_xgb))\n",
        "\n",
        "# Compute ROC-AUC score\n",
        "roc_auc = roc_auc_score(y_test, xgb_clf.predict_proba(X_test)[:,1])\n",
        "print(\"ROC-AUC Score:\", roc_auc)\n"
      ],
      "metadata": {
        "id": "dhHmAgkL6yeV"
      },
      "id": "dhHmAgkL6yeV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Looking good, but lets double check"
      ],
      "metadata": {
        "id": "woq7McYmWfzF"
      },
      "id": "woq7McYmWfzF"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import cross_val_predict\n",
        "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, roc_auc_score\n",
        "\n",
        "# Get cross-validated estimates for each data point\n",
        "y_pred = cross_val_predict(clf, X_train_smote, y_train_smote, cv=5)\n",
        "\n",
        "# Compute confusion matrix\n",
        "conf_mat = confusion_matrix(y_train_smote, y_pred)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_mat)\n",
        "\n",
        "# Calculate precision, recall, and F1 score\n",
        "precision = precision_score(y_train_smote, y_pred)\n",
        "recall = recall_score(y_train_smote, y_pred)\n",
        "f1 = f1_score(y_train_smote, y_pred)\n",
        "\n",
        "print(f\"Precision: {precision}\")\n",
        "print(f\"Recall: {recall}\")\n",
        "print(f\"F1 Score: {f1}\")\n",
        "\n",
        "# Compute ROC-AUC\n",
        "# ROC-AUC might require probability scores instead of binary predictions, depending on your use case\n",
        "y_scores = cross_val_predict(clf, X_train_smote, y_train_smote, cv=5, method='predict_proba')\n",
        "roc_auc = roc_auc_score(y_train_smote, y_scores[:, 1])  # Assuming the positive class is labeled '1'\n",
        "\n",
        "print(f\"ROC-AUC Score: {roc_auc}\")\n"
      ],
      "metadata": {
        "id": "09s1aYjwVXv6"
      },
      "id": "09s1aYjwVXv6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Assuming X_train_smote and y_train_smote are defined\n",
        "clf = RandomForestClassifier()\n",
        "clf.fit(X_train_smote, y_train_smote)\n",
        "\n",
        "# Get feature importances\n",
        "importances = clf.feature_importances_\n",
        "feature_names = X_train_smote.columns\n",
        "\n",
        "# Sort feature importances in descending order and plot\n",
        "indices = np.argsort(importances)[::-1]\n",
        "\n",
        "plt.figure()\n",
        "plt.title(\"Feature importances\")\n",
        "plt.bar(range(X_train_smote.shape[1]), importances[indices])\n",
        "plt.xticks(range(X_train_smote.shape[1]), feature_names[indices], rotation=90)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "W5EG1NkXKyk0"
      },
      "id": "W5EG1NkXKyk0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Model Architecture"
      ],
      "metadata": {
        "id": "qzn6UlrgXVHU"
      },
      "id": "qzn6UlrgXVHU"
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Model log:\n",
        "Model 1:\n",
        "- loss: 0.4380\n",
        "- accuracy: 0.7573\n",
        "- Test Loss: 0.43803343176841736\n",
        "- Test Accuracy: 0.7573385238647461\n",
        "\n",
        "Model 1.2: used binning to solve issues w age, blood glucose and bmi\n",
        "- loss: 0.5950\n",
        "- accuracy: 0.5564\n",
        "- Test Loss: 0.59500360488891\n",
        "- Test Accuracy: 0.5564253330230713.\n",
        "\n",
        "- Precision: 0.10252996005326231\n",
        "- Recall: 0.8651685393258427\n",
        "- F1-Score: 0.18333333333333332\n",
        "- AUC-ROC: 0.766799464658097\n",
        "\n",
        "model 1.3: bayesian hypertuning, optimized model layering structure\n",
        "- loss: 0.4934\n",
        "- accuracy: 0.8108\n",
        "- Test Loss: 0.49342384934425354\n",
        "- Test Accuracy: 0.810828447341919\n",
        "- Accuracy: 0.8101761252446184\n",
        "- Precision: 0.13405797101449277\n",
        "- Recall: 0.4157303370786517\n",
        "- F1 Score: 0.2027397260273973\n",
        "- Confusion Matrix:\n",
        "  [[1205  239]\n",
        "  [  52   37]]\n",
        "- ROC AUC Score: 0.7156035046219926\n",
        "\n",
        "model 1.3.1: ensemble with 1.3\n",
        "\n"
      ],
      "metadata": {
        "id": "YniID-eDce3q"
      },
      "id": "YniID-eDce3q"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip list | grep tensorflow\n"
      ],
      "metadata": {
        "id": "wQZAvGm3Db12"
      },
      "id": "wQZAvGm3Db12",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade tensorflow tensorflow-datasets tensorflow-estimator tensorflow-gcs-config tensorflow-hub tensorflow-io-gcs-filesystem tensorflow-metadata tensorflow-probability\n"
      ],
      "metadata": {
        "id": "oA9O9EWwE_br"
      },
      "id": "oA9O9EWwE_br",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bayesian tuning to produce new fixed model"
      ],
      "metadata": {
        "id": "MY1imigKqg1g"
      },
      "id": "MY1imigKqg1g"
    },
    {
      "cell_type": "code",
      "source": [
        "import optuna\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "\n",
        "# Function to create model for Optuna\n",
        "def create_fixed_model():\n",
        "    model = Sequential()\n",
        "    # Using the best parameters from Keras Tuner\n",
        "    model.add(Dense(416, activation='relu', input_shape=(X_train_smote.shape[1],)))  # units_input from Keras Tuner\n",
        "    model.add(Dropout(0.1))  # dropout_input from Keras Tuner\n",
        "\n",
        "    # Additional layers based on the best configuration found\n",
        "    num_layers = 1  # from num_layers in Keras Tuner\n",
        "    if num_layers > 0:\n",
        "        model.add(Dense(384, activation='relu'))  # units_layer_0 from Keras Tuner\n",
        "        model.add(Dropout(0.2))  # dropout_layer_0 from Keras Tuner\n",
        "\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    learning_rate = 10 ** (-3.201883719724095)  # Convert log_learning_rate to actual learning rate\n",
        "    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Objective function for Optuna\n",
        "def objective(trial):\n",
        "    model = create_model(trial)\n",
        "    callbacks = [\n",
        "        EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True),\n",
        "        ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, min_lr=1e-6)\n",
        "    ]\n",
        "    history = model.fit(\n",
        "        X_train_smote, y_train_smote,\n",
        "        validation_data=(X_val, y_val),\n",
        "        epochs=100, batch_size=32,\n",
        "        callbacks=callbacks,\n",
        "        verbose=0\n",
        "    )\n",
        "    val_accuracy = model.evaluate(X_val, y_val, verbose=1)[1]\n",
        "    return val_accuracy\n",
        "\n",
        "# Create a study object and specify the direction is 'maximize'.\n",
        "study = optuna.create_study(direction='maximize')\n",
        "study.optimize(objective, n_trials=10)\n",
        "best_trial = study.best_trial\n",
        "print(f'Accuracy: {best_trial.value}')\n",
        "print(f\"Best hyperparameters: {best_trial.params}\")\n"
      ],
      "metadata": {
        "id": "TtnJEIS-VXyz"
      },
      "id": "TtnJEIS-VXyz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keras-tuner\n"
      ],
      "metadata": {
        "id": "2rGoid1k4co3"
      },
      "id": "2rGoid1k4co3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hypetuning the bayesian model\n"
      ],
      "metadata": {
        "id": "S-H-UExLqoLA"
      },
      "id": "S-H-UExLqoLA"
    },
    {
      "cell_type": "code",
      "source": [
        "import optuna\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "\n",
        "# Define a fixed model with best hyperparameters from previous testing\n",
        "def create_fixed_model(trial):\n",
        "    model = Sequential()\n",
        "    # Base configuration from previous Bayesian optimization\n",
        "    model.add(Dense(416, activation='relu', input_shape=(X_train_smote.shape[1],)))\n",
        "    model.add(Dropout(0.1))\n",
        "    model.add(Dense(384, activation='relu'))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "    # Tunable learning rate\n",
        "    learning_rate = trial.suggest_float('learning_rate', 1e-4, 1e-2)\n",
        "    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Objective function for Optuna to fine-tune other parameters\n",
        "def objective(trial):\n",
        "    model = create_fixed_model(trial)\n",
        "    batch_size = trial.suggest_categorical('batch_size', [16, 32, 64, 128])\n",
        "\n",
        "    callbacks = [\n",
        "        EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True),\n",
        "        ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=2, min_lr=1e-6)\n",
        "    ]\n",
        "\n",
        "    history = model.fit(\n",
        "        X_train_smote, y_train_smote,\n",
        "        validation_data=(X_val, y_val),\n",
        "        epochs=50,  # Optimize for quicker iterations\n",
        "        batch_size=batch_size,\n",
        "        callbacks=callbacks,\n",
        "        verbose=0\n",
        "    )\n",
        "\n",
        "    val_accuracy = model.evaluate(X_val, y_val, verbose=0)[1]\n",
        "    return val_accuracy\n",
        "\n",
        "# Create and run the study\n",
        "study = optuna.create_study(direction='maximize')\n",
        "study.optimize(objective, n_trials=20)  # Increase trials if needed\n",
        "best_trial = study.best_trial\n",
        "\n",
        "print(f'Best Validation Accuracy: {best_trial.value}')\n",
        "print(f\"Best hyperparameters: {best_trial.params}\")\n"
      ],
      "metadata": {
        "id": "K2YMVsh9qoTZ"
      },
      "id": "K2YMVsh9qoTZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "qimSB7Y4yza3"
      },
      "id": "qimSB7Y4yza3"
    },
    {
      "cell_type": "code",
      "source": [
        "# from tensorflow.keras.models import Sequential\n",
        "# from tensorflow.keras.layers import Dense, Dropout\n",
        "# from tensorflow.keras.optimizers import Adam\n",
        "# from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "# #untuned 48/48 [==============================] - 0s 2ms/step - loss: 0.5950 - accuracy: 0.5564\n",
        "# #Test Loss: 0.595003604888916, Test Accuracy: 0.5564253330230713\n",
        "# # 48/48 [==============================] - 0s 2ms/step - loss: 0.5038 - accuracy: 0.8082\n",
        "# # Test Loss: 0.5038195848464966, Test Accuracy: 0.8082191944122314\n",
        "\n",
        "\n",
        "# # Building the model with optimal parameters\n",
        "\n",
        "\n",
        "# # Best hyperparameters\n",
        "# #hyperparameters = {\n",
        "#     'n_layers': 3,\n",
        "#     'dropout_rate': 0.1931642791147507,\n",
        "#     'lr': 0.0003865222724920752,\n",
        "#     'n_units_first': 74,\n",
        "#     'n_units_l0': 77,\n",
        "#     'n_units_l1': 119,\n",
        "#     'n_units_l2': 27\n",
        "# }\n",
        "\n",
        "# # Define the model with hyperparameters\n",
        "# model = Sequential([\n",
        "#     Dense(hyperparameters['n_units_first'], activation='relu', input_shape=(X_train_smote.shape[1],)),  # First layer\n",
        "#     Dense(hyperparameters['n_units_l0'], activation='relu'),  # Second layer\n",
        "#     Dense(hyperparameters['n_units_l1'], activation='relu'),  # Third layer\n",
        "#     Dropout(hyperparameters['dropout_rate']),  # Dropout layer\n",
        "#     Dense(hyperparameters['n_units_l2'], activation='relu'),  # Fourth layer\n",
        "#     Dense(1, activation='sigmoid')  # Output layer\n",
        "# ])\n",
        "\n",
        "# # Compile the model with the optimal learning rate\n",
        "# model.compile(optimizer=Adam(learning_rate=hyperparameters['lr']),\n",
        "#               loss='binary_crossentropy',\n",
        "#               metrics=['accuracy'])\n",
        "\n",
        "# # Early stopping to prevent overfitting\n",
        "# early_stopping = EarlyStopping(\n",
        "#     monitor='val_loss',\n",
        "#     patience=10,\n",
        "#     restore_best_weights=True\n",
        "# )\n",
        "\n",
        "# # Adjust ReduceLROnPlateau to be more patient\n",
        "# reduce_lr = ReduceLROnPlateau(\n",
        "#     monitor='val_loss',\n",
        "#     factor=0.1,  # Reduce the learning rate by a factor of 0.1\n",
        "#     patience=5,  # Increased patience\n",
        "#     min_lr=1e-6  # Lower bound on the learning rate\n",
        "# )\n",
        "\n",
        "# # Train the model with the SMOTE-augmented training data and callbacks\n",
        "# history = model.fit(\n",
        "#     X_train_smote, y_train_smote,\n",
        "#     epochs=100,\n",
        "#     batch_size=32,\n",
        "#     validation_data=(X_val, y_val),\n",
        "#     callbacks=[early_stopping, reduce_lr]\n",
        "# )\n",
        "\n",
        "# # Model summary\n",
        "# model.summary()\n"
      ],
      "metadata": {
        "id": "DziX6VUqYZ75"
      },
      "id": "DziX6VUqYZ75",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
        "import os\n",
        "\n",
        "# Set the best hyperparameters from Optuna\n",
        "best_learning_rate = 0.002363396022137484\n",
        "best_batch_size = 64\n",
        "\n",
        "# Build the model with the optimal hyperparameters\n",
        "def build_final_model():\n",
        "    model = Sequential()\n",
        "    model.add(Dense(416, activation='relu', input_shape=(X_train_smote.shape[1],)))\n",
        "    model.add(Dropout(0.1))\n",
        "    model.add(Dense(384, activation='relu'))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    model.compile(optimizer=Adam(learning_rate=best_learning_rate), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Create the model\n",
        "model = build_final_model()\n",
        "\n",
        "# Callbacks for early stopping and learning rate reduction\n",
        "callbacks = [\n",
        "    EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True),\n",
        "    ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=2, min_lr=1e-6),\n",
        "    ModelCheckpoint(filepath='best_model.h5', monitor='val_accuracy', save_best_only=True)\n",
        "]\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    X_train_smote, y_train_smote,\n",
        "    validation_data=(X_val, y_val),\n",
        "    epochs=100,  # Set a higher epoch if needed\n",
        "    batch_size=best_batch_size,\n",
        "    callbacks=callbacks,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
        "print(f\"Test Loss: {test_loss}, Test Accuracy: {test_accuracy}\")\n",
        "\n",
        "# Load the best model\n",
        "best_model = tf.keras.models.load_model('best_model.h5')\n"
      ],
      "metadata": {
        "id": "bYqYK7070UAk"
      },
      "id": "bYqYK7070UAk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
        "print(f\"Test Loss: {test_loss}, Test Accuracy: {test_acc}\")\n"
      ],
      "metadata": {
        "id": "VNX1JRB3ao6T"
      },
      "id": "VNX1JRB3ao6T",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_auc_score, roc_curve\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Load the best model (already loaded as `best_model` in the previous step)\n",
        "best_model = tf.keras.models.load_model('best_model.h5')\n",
        "\n",
        "# Predict probabilities for the test set\n",
        "y_probs = best_model.predict(X_test)\n",
        "y_pred = np.round(y_probs)\n",
        "\n",
        "# Compute evaluation metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "roc_auc = roc_auc_score(y_test, y_probs)\n",
        "\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "print(f\"Precision: {precision}\")\n",
        "print(f\"Recall: {recall}\")\n",
        "print(f\"F1 Score: {f1}\")\n",
        "print(f\"Confusion Matrix:\\n{conf_matrix}\")\n",
        "print(f\"ROC AUC Score: {roc_auc}\")\n",
        "\n",
        "# Plot ROC Curve\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_probs)\n",
        "plt.figure()\n",
        "plt.plot(fpr, tpr, label=f'ROC curve (area = {roc_auc:.2f})')\n",
        "plt.plot([0, 1], [0, 1], 'k--')  # Dashed diagonal\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curve')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "fNB720xp05Rh"
      },
      "id": "fNB720xp05Rh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "7WWFWBGQzaUu"
      },
      "id": "7WWFWBGQzaUu"
    },
    {
      "cell_type": "code",
      "source": [
        "# import numpy as np\n",
        "# from sklearn.model_selection import StratifiedKFold\n",
        "# from tensorflow.keras.models import Sequential\n",
        "# from tensorflow.keras.layers import Dense, Dropout\n",
        "# from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# # Assuming X_train_smote and y_train_smote are pandas DataFrame and Series respectively\n",
        "# def build_model():\n",
        "#     model = Sequential([\n",
        "#         Dense(83, activation='relu', input_shape=(X_train_smote.shape[1],)),  # Adjust input_shape to match feature size\n",
        "#         Dense(121, activation='relu'),\n",
        "#         Dropout(0.3475302741733841),  # Adjust dropout rate if necessary\n",
        "#         Dense(1, activation='sigmoid')\n",
        "#     ])\n",
        "#     model.compile(optimizer=Adam(learning_rate=0.07826801938092297),\n",
        "#                   loss='binary_crossentropy',\n",
        "#                   metrics=['accuracy'])\n",
        "#     return model\n",
        "\n",
        "# n_splits = 5\n",
        "# kfold = StratifiedKFold(n_splits=n_splits, shuffle=True)\n",
        "\n",
        "# scores = []\n",
        "# for train, test in kfold.split(X_train_smote, y_train_smote):\n",
        "#     model = build_model()\n",
        "#     # Use .iloc for proper indexing when using pandas data structures\n",
        "#     model.fit(X_train_smote.iloc[train], y_train_smote.iloc[train], epochs=100, batch_size=32, verbose=0)\n",
        "#     score = model.evaluate(X_train_smote.iloc[test], y_train_smote.iloc[test], verbose=0)\n",
        "#     scores.append(score)\n",
        "\n",
        "# # Print the cross-validation scores\n",
        "# print(f\"Cross-validated scores: {scores}\")\n"
      ],
      "metadata": {
        "id": "RizZnPBzzZKG"
      },
      "id": "RizZnPBzzZKG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot training & validation accuracy values\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('Model accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "# Plot training & validation loss values\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "rytnusiMX9up"
      },
      "id": "rytnusiMX9up",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Error Analysis"
      ],
      "metadata": {
        "id": "_03e6G4DdIZA"
      },
      "id": "_03e6G4DdIZA"
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the predicted probabilities\n",
        "y_pred_probs = model.predict(X_test)\n",
        "\n",
        "# Convert probabilities to binary predictions using a threshold (default is 0.5)\n",
        "y_pred = (y_pred_probs > 0.5).astype(int)\n",
        "\n",
        "# Get the actual predictions and the indices where the predictions were incorrect\n",
        "incorrect_indices = np.where(y_pred.flatten() != y_test)[0]\n"
      ],
      "metadata": {
        "id": "Q7eIt4fZann6"
      },
      "id": "Q7eIt4fZann6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a DataFrame to compare actual labels and predicted probabilities/predictions\n",
        "errors_df = pd.DataFrame({'Actual': y_test, 'Predicted_Prob': y_pred_probs.flatten(), 'Predicted': y_pred.flatten()})\n",
        "errors_df['Error'] = errors_df['Actual'] != errors_df['Predicted']\n",
        "\n",
        "# Extract the subset of the DataFrame where predictions are incorrect\n",
        "misclassified = errors_df[errors_df['Error']]\n",
        "\n",
        "# Filter out the misclassified cases\n",
        "misclassified = errors_df[errors_df['Error']]\n",
        "\n",
        "# Analyze the distribution of probabilities for misclassified cases\n",
        "sns.histplot(misclassified['Predicted_Prob'], bins=30, kde=False)\n",
        "plt.title('Distribution of Predicted Probabilities for Misclassified Cases')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Iu3DWsmJX9w1"
      },
      "id": "Iu3DWsmJX9w1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cases where the model was very wrong\n",
        "high_confidence_errors = misclassified[(misclassified['Predicted_Prob'] > 0.9) | (misclassified['Predicted_Prob'] < 0.1)]\n",
        "X_test_errors = X_test.loc[high_confidence_errors.index]\n",
        "\n",
        "# Check for common features among high confidence errors\n",
        "common_features = X_test_errors.mean() - X_test.mean()\n",
        "\n",
        "# This would give you the top 5 features with the highest divergence\n",
        "top_divergent_features = common_features.abs().sort_values(ascending=False).head(5).index.tolist()\n",
        "print(top_divergent_features)\n"
      ],
      "metadata": {
        "id": "L3zyqFYiX9ys"
      },
      "id": "L3zyqFYiX9ys",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for feature in top_divergent_features:\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    sns.histplot(X_test_errors[feature], color='red', label='High Confidence Errors', kde=True)\n",
        "    sns.histplot(X_test[feature], color='blue', label='All Test Data', kde=True)\n",
        "    plt.title(f'Distribution of {feature} for All Test Data vs High Confidence Errors')\n",
        "    plt.xlabel(feature)\n",
        "    plt.ylabel('Density')\n",
        "    plt.legend()\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "NxhbmT1meHQi"
      },
      "id": "NxhbmT1meHQi",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming 'high_confidence_errors' is a DataFrame with cases where the model was very wrong\n",
        "for feature in common_features.sort_values(key=abs, ascending=False).index[:5]:  # Top 5 features\n",
        "    plt.figure(figsize=(10, 4))\n",
        "    sns.kdeplot(X_test_errors[feature], label='Errors', fill=True)\n",
        "    sns.kdeplot(X_test[feature], label='All Test Data', fill=True)\n",
        "    plt.title(f'Distribution of {feature} for All Test Data vs Errors')\n",
        "    plt.xlabel(feature)\n",
        "    plt.ylabel('Density')\n",
        "    plt.legend()\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "nGHrwh0hf0vO"
      },
      "id": "nGHrwh0hf0vO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score\n",
        "\n",
        "# Assume y_test are the true labels and model_predictions are the predictions from the model\n",
        "# Replace model.predict(X_test) with your model's prediction method\n",
        "model_predictions = (model.predict(X_test) > 0.5).astype(\"int32\")\n",
        "\n",
        "precision = precision_score(y_test, model_predictions)\n",
        "recall = recall_score(y_test, model_predictions)\n",
        "f1 = f1_score(y_test, model_predictions)\n",
        "roc_auc = roc_auc_score(y_test, model.predict(X_test))\n",
        "\n",
        "print(f\"Precision: {precision}\")\n",
        "print(f\"Recall: {recall}\")\n",
        "print(f\"F1-Score: {f1}\")\n",
        "print(f\"AUC-ROC: {roc_auc}\")\n"
      ],
      "metadata": {
        "id": "afLyL7qvf0xk"
      },
      "id": "afLyL7qvf0xk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ADmOs_12f0zc"
      },
      "id": "ADmOs_12f0zc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "okSSLYOIf01d"
      },
      "id": "okSSLYOIf01d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8bMQEQ8-eHTp"
      },
      "id": "8bMQEQ8-eHTp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TMWJpZpreHV_"
      },
      "id": "TMWJpZpreHV_",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}